import os
import faiss
import pickle
import numpy as np
from openai import AzureOpenAI
import random

class AgenticGenerator:
    def __init__(self, prompt, chunks):
        self.prompt = prompt
        self.chunks = chunks
        self.results = []
        self.index_file = "faiss_index.bin"
        self.data_file = "text_chunks.pkl"

        # Azure OpenAI setup
        self.client = AzureOpenAI(
            api_version=os.getenv("AZURE_API_VERSION"),
            azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
            api_key=os.getenv("AZURE_OPENAI_API_KEY")
        )
        self.deployment_model = os.getenv("AZURE_TEST_MODEL")
        self.embedding_model = os.getenv("AZURE_EMBEDDING_MODEL")

        # Load or initialize FAISS
        self.load_faiss()

        # Initialize Agent to take decisions autonomously
        self.agent = Agent(self)

    def load_faiss(self):
        if os.path.exists(self.index_file) and os.path.exists(self.data_file):
            self.index = faiss.read_index(self.index_file)
            with open(self.data_file, "rb") as f:
                self.stored_texts = pickle.load(f)
        else:
            # Initialize FAISS for 3072-dimensional embeddings
            self.index = faiss.IndexFlatL2(3072)  # 3072 dimensions for embeddings
            self.stored_texts = []

    def save_faiss(self):
        faiss.write_index(self.index, self.index_file)
        with open(self.data_file, "wb") as f:
            pickle.dump(self.stored_texts, f)

    def get_embedding(self, text):
        response = self.client.embeddings.create(
            input=[text],
            model=self.embedding_model
        )
        embedding = np.array(response.data[0].embedding, dtype=np.float32)
        return embedding

    def store_chunk(self, chunk):
        embedding = self.get_embedding(chunk)
        embedding = np.array([embedding], dtype=np.float32)  # FAISS requires a 2D array
        self.index.add(embedding)  # Add embedding to FAISS index
        self.stored_texts.append(chunk)
        self.save_faiss()

    def get_similar_chunks(self, query_text, top_k=3):
        if len(self.stored_texts) == 0:
            return ""
        embedding = self.get_embedding(query_text)
        D, I = self.index.search(np.array([embedding]), top_k)
        return "\n".join([self.stored_texts[i] for i in I[0] if i < len(self.stored_texts)])

    def llms(self, text):
        similar_chunks = self.get_similar_chunks(text)
        prompt = f"""
        {self.prompt}
        
        Here are some similar past user stories for your reference:
        {similar_chunks}
        
        Now based on the above references, answer for:
        {{ {text} }}
        """
        messages = [{"role": "user", "content": prompt}]
        response = self.client.chat.completions.create(
            model=self.deployment_model,
            messages=messages,
            temperature=0,
        )
        return response.choices[0].message.content

    def gn_Case(self):
        for chunk in self.chunks:
            result = self.agent.act(chunk)  # Let the agent decide and act
            if result:
                self.results.append(result)
                self.store_chunk(chunk)  # Store the chunk in FAISS as needed
        return self.results


class Agent:
    def __init__(self, generator):
        self.generator = generator

    def decide_action(self, query):
        """
        Example decision logic for autonomous task execution.
        If the query is too new and there's no similar data, we fetch new data.
        Otherwise, we generate a response using LLM.
        """
        similar_chunks = self.generator.get_similar_chunks(query)
        if not similar_chunks:
            return "fetch new data"  # If no similar data, we fetch new data
        return "generate response"  # If we have similar data, we generate a response

    def act(self, query):
        """
        The agent acts based on its decision.
        It will either fetch new data (store the query) or generate a response using the LLM.
        """
        action = self.decide_action(query)
        if action == "fetch new data":
            # Fetch new data by storing the query (adding to FAISS)
            self.generator.store_chunk(query)
            return f"New data fetched and stored: {query}"
        elif action == "generate response":
            # Generate a response using LLM
            return self.generator.llms(query)
        return "No valid action"


# Example Usage:
if __name__ == "__main__":
    chunks = [
        "AI is disrupting traditional industries.",
        "Machine learning is revolutionizing healthcare.",
        "Opportunities in AI are growing exponentially."
    ]
    prompt = "Based on the given data, generate a response to the query."

    # Initialize the AgenticGenerator
    agent = AgenticGenerator(prompt, chunks)

    # Call the gn_Case method to process the chunks and get the results
    results = agent.gn_Case()

    # Print the results
    print("Generated Responses and Actions Taken:")
    for result in results:
        print(result)
