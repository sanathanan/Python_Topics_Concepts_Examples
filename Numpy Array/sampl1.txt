import os
import faiss
import pickle
import numpy as np
from openai import AzureOpenAI

class Generator:
    def __init__(self, prompt, chunks):
        self.prompt = prompt
        self.chunks = chunks
        self.results = []
        self.index_file = "faiss_index.bin"
        self.data_file = "text_chunks.pkl"
        
        # Azure OpenAI setup
        self.client = AzureOpenAI(
            api_version=os.getenv("AZURE_API_VERSION"),
            azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
            api_key=os.getenv("AZURE_OPENAI_API_KEY")
        )
        self.deployment_model = os.getenv("AZURE_TEST_MODEL")
        self.embedding_model = os.getenv("AZURE_EMBEDDING_MODEL")
        
        # Load or initialize FAISS
        self.load_faiss()

    def load_faiss(self):
        if os.path.exists(self.index_file) and os.path.exists(self.data_file):
            self.index = faiss.read_index(self.index_file)
            with open(self.data_file, "rb") as f:
                self.stored_texts = pickle.load(f)
        else:
            # Initialize FAISS for 3072-dimensional embeddings
            self.index = faiss.IndexFlatL2(3072)  # 3072 dimensions for embeddings
            self.stored_texts = []

    def save_faiss(self):
        faiss.write_index(self.index, self.index_file)
        with open(self.data_file, "wb") as f:
            pickle.dump(self.stored_texts, f)

    def get_embedding(self, text):
        response = self.client.embeddings.create(
            input=[text],
            model=self.embedding_model
        )
        embedding = np.array(response.data[0].embedding, dtype=np.float32)
        
        # Debug: Print the shape of the embedding
        print("Embedding shape:", embedding.shape)
        
        return embedding

    def store_chunk(self, chunk):
        # Get the embedding for the chunk
        embedding = self.get_embedding(chunk)
        
        # Ensure the embedding is in the correct shape for FAISS (2D array)
        embedding = np.array([embedding], dtype=np.float32)  # FAISS requires a 2D array
        
        # Debug: Print the shape before adding to FAISS
        print("Embedding shape before adding to FAISS:", embedding.shape)
        
        # Add the embedding to the FAISS index
        self.index.add(embedding)  # Add embedding to FAISS index
        
        # Store the text chunk in the list
        self.stored_texts.append(chunk)
        
        # Save the FAISS index and text chunks to disk
        self.save_faiss()

    def get_similar_chunks(self, query_text, top_k=3):
        if len(self.stored_texts) == 0:
            return ""
        embedding = self.get_embedding(query_text)
        D, I = self.index.search(np.array([embedding]), top_k)
        return "\n".join([self.stored_texts[i] for i in I[0] if i < len(self.stored_texts)])

    def llms(self, text):
        similar_chunks = self.get_similar_chunks(text)
        
        prompt = f"""
        {self.prompt}
        
        Here are some similar past user stories for your reference:
        {similar_chunks}
        
        Now based on the above references, answer for:
        {{ {text} }}
        """
        
        messages = [{"role": "user", "content": prompt}]
        response = self.client.chat.completions.create(
            model=self.deployment_model,
            messages=messages,
            temperature=0,
        )
        
        return response.choices[0].message.content

    def gn_Case(self):
        for chunk in self.chunks:
            result = self.llms(chunk)
            self.results.append(result)
            self.store_chunk(chunk)
        return self.results
